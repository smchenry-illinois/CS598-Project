{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce results of Scheme B\n",
    "\n",
    "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
    "\n",
    "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
    "\n",
    "February 2019\n",
    "\n",
    "Jounal of Biomedical Informatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = 'ePBRN_F_dup' \n",
    "testset = 'ePBRN_D_dup'\n",
    "\n",
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    unique_match_id = unique_match_id[~np.isnan(unique_match_id)] # remove nan values\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return ((f11 == f22) & (f12 == f21)).astype(float)\n",
    "\n",
    "def join_names_space(f11, f12, f21, f22):\n",
    "    return ((f11+\" \"+f12 == f21) | (f11+\" \"+f12 == f22)| (f21+\" \"+f22 == f11)| (f21+\" \"+f22 == f12)).astype(float)\n",
    "\n",
    "def join_names_dash(f11, f12, f21, f22):\n",
    "    return ((f11+\"-\"+f12 == f21) | (f11+\"-\"+f12 == f22)| (f21+\"-\"+f22 == f11)| (f21+\"-\"+f22 == f12)).astype(float)\n",
    "\n",
    "def abb_surname(f1, f2):\n",
    "    return ((f1[0]==f2) | (f1==f2[0])).astype(float)\n",
    "\n",
    "def reset_day(f11, f12, f21, f22):\n",
    "    return (((f11 == 1) & (f12 == 1))|((f21 == 1) & (f22 == 1))).astype(float)\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='levenshtein', label='y_name_leven')\n",
    "    c.string('surname', 'surname', method='levenshtein', label='y_surname_leven')  \n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name_jaro')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname_jaro')  \n",
    "    c.string('postcode', 'postcode', method='jarowinkler', label='y_postcode')      \n",
    "    exact_fields = ['postcode', 'address_1', 'address_2', 'street_number']\n",
    "    for field in exact_fields:\n",
    "        c.exact(field, field, label='y_'+field+'_exact')\n",
    "    c.compare_vectorized(reset_day,('day', 'month'), ('day', 'month'),label='reset_day_flag')    \n",
    "    c.compare_vectorized(swap_fields_flag,('day', 'month'), ('day', 'month'),label='swap_day_month')    \n",
    "    c.compare_vectorized(swap_fields_flag,('surname', 'given_name'), ('surname', 'given_name'),label='swap_names')    \n",
    "    c.compare_vectorized(join_names_space,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_space')\n",
    "    c.compare_vectorized(join_names_dash,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_dash')\n",
    "    c.compare_vectorized(abb_surname,'surname', 'surname',label='abb_surname')\n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'svm': # Support Vector Machine\n",
    "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nb': # Naive Bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=30000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 14078 , number of matched pairs:  3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\recordlinkage\\preprocessing\\encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building X_train, y_train\n"
     ]
    }
   ],
   "source": [
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import test set...\n",
      "Test set size: 11731 , number of matched pairs:  2653\n",
      "BLOCKING PERFORMANCE:\n",
      "Number of pairs of matched given_name: 252552 , detected  1567 /2653 true matched pairs, missed 1086\n",
      "Number of pairs of matched surname: 33832 , detected  1480 /2653 true matched pairs, missed 1173\n",
      "Number of pairs of matched postcode: 79940 , detected  2462 /2653 true matched pairs, missed 191\n",
      "Number of pairs of at least 1 field matched: 362910 , detected  2599 /2653 true matched pairs, missed 54\n"
     ]
    }
   ],
   "source": [
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "print(\"Import test set...\")\n",
    "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
    "\n",
    "print(\"BLOCKING PERFORMANCE:\")\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
    "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
    "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n",
      "Count labels of y_test: Counter({0: 360311, 1: 2599})\n",
      "Finished building X_test, y_test\n"
     ]
    }
   ],
   "source": [
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
      "Model: svm , Param_1: rbf , tuning range: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
      "No_false: [5468, 9207, 76204, 81599, 81385, 78545, 78233, 78568, 79306, 83314, 92616, 87369, 85390, 86085, 84369, 86029, 76675, 75894, 76380, 76731] \n",
      "\n",
      "Precision: [0.32048554623951947, 0.21909539194014624, 0.032957271031358266, 0.030856572758800892, 0.03093520040008573, 0.032017943851519556, 0.03214153160955091, 0.03200887081870264, 0.031731884500335754, 0.030251533528104012, 0.027296119308932415, 0.02887757597314541, 0.029527089229090663, 0.029285069914298602, 0.029862931787866243, 0.029303574652464345, 0.032761448214961526, 0.03308743900419167, 0.032883823994935106, 0.032738320348939816] \n",
      "\n",
      "Sensitivity: [0.9853789919199692, 0.9915352058484033, 0.9992304732589458, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 1.0, 1.0, 1.0, 0.9996152366294728, 0.9996152366294728, 0.9992304732589458, 0.9992304732589458, 0.9992304732589458, 0.9992304732589458, 0.9992304732589458, 0.9992304732589458, 0.9992304732589458] \n",
      "\n",
      "F-score: [0.48366383380547684, 0.3588886567787759, 0.06380992161969581, 0.059865199608272364, 0.06001316686108961, 0.062048458938871044, 0.062280501983722696, 0.062031421613103474, 0.06151188109438606, 0.0587265003615329, 0.05314167706054347, 0.0561335277912818, 0.057359856931534677, 0.05690246387449467, 0.05799269787747173, 0.05693739517446259, 0.06344281718354933, 0.06405386740331492, 0.06367224851055484, 0.0633994507171193] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "# Choose model\n",
    "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
    "modeltype = 'svm' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000] # C for svm, C for lg, alpha for NN\n",
    "\n",
    "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
    "precision = []\n",
    "sensitivity = []\n",
    "Fscore = []\n",
    "nb_false = []\n",
    "\n",
    "for modelparam in modelparam_range:\n",
    "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision += [final_eval['precision']]\n",
    "    sensitivity += [final_eval['sensitivity']]\n",
    "    Fscore += [final_eval['F-score']]\n",
    "    nb_false  += [final_eval['no_false']]\n",
    "    \n",
    "print(\"No_false:\",nb_false,\"\\n\")\n",
    "print(\"Precision:\",precision,\"\\n\")\n",
    "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
    "print(\"F-score:\", Fscore,\"\\n\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAGGING PERFORMANCE:\n",
      "\n",
      "svm per fold:\n",
      "Fold 0 {'no_false': 4356, 'confusion_matrix': [2544, 4301, 55, 356010], 'precision': 0.3716581446311176, 'sensitivity': 0.9788380146210081, 'no_links': 6845, 'F-score': 0.5387547649301144}\n",
      "Fold 1 {'no_false': 4032, 'confusion_matrix': [2544, 3977, 55, 356334], 'precision': 0.39012421407759545, 'sensitivity': 0.9788380146210081, 'no_links': 6521, 'F-score': 0.5578947368421053}\n",
      "Fold 2 {'no_false': 4192, 'confusion_matrix': [2546, 4139, 53, 356172], 'precision': 0.38085265519820494, 'sensitivity': 0.9796075413620623, 'no_links': 6685, 'F-score': 0.5484704868591125}\n",
      "Fold 3 {'no_false': 4233, 'confusion_matrix': [2544, 4178, 55, 356133], 'precision': 0.3784587920261827, 'sensitivity': 0.9788380146210081, 'no_links': 6722, 'F-score': 0.5458641776633409}\n",
      "Fold 4 {'no_false': 4203, 'confusion_matrix': [2541, 4145, 58, 356166], 'precision': 0.3800478612025127, 'sensitivity': 0.9776837245094268, 'no_links': 6686, 'F-score': 0.5473344103392569}\n",
      "Fold 5 {'no_false': 4117, 'confusion_matrix': [2544, 4062, 55, 356249], 'precision': 0.3851044504995459, 'sensitivity': 0.9788380146210081, 'no_links': 6606, 'F-score': 0.5527430744160783}\n",
      "Fold 6 {'no_false': 4088, 'confusion_matrix': [2542, 4031, 57, 356280], 'precision': 0.3867336071808915, 'sensitivity': 0.9780684878799538, 'no_links': 6573, 'F-score': 0.554295682511993}\n",
      "Fold 7 {'no_false': 4002, 'confusion_matrix': [2541, 3944, 58, 356367], 'precision': 0.39182729375481884, 'sensitivity': 0.9776837245094268, 'no_links': 6485, 'F-score': 0.559445178335535}\n",
      "Fold 8 {'no_false': 4259, 'confusion_matrix': [2544, 4204, 55, 356107], 'precision': 0.3770005927682276, 'sensitivity': 0.9788380146210081, 'no_links': 6748, 'F-score': 0.5443457793944582}\n",
      "Fold 9 {'no_false': 4120, 'confusion_matrix': [2541, 4062, 58, 356249], 'precision': 0.38482507950931394, 'sensitivity': 0.9776837245094268, 'no_links': 6603, 'F-score': 0.5522712453814388}\n",
      "svm bagging: {'no_false': 4136, 'confusion_matrix': [2544, 4081, 55, 356230], 'precision': 0.384, 'sensitivity': 0.9788380146210081, 'no_links': 6625, 'F-score': 0.5516045099739809}\n",
      "\n",
      "nn per fold:\n",
      "Fold 0 {'no_false': 1184, 'confusion_matrix': [2506, 1091, 93, 359220], 'precision': 0.6966916875173756, 'sensitivity': 0.9642170065409773, 'no_links': 3597, 'F-score': 0.8089089735313105}\n",
      "Fold 1 {'no_false': 1037, 'confusion_matrix': [2506, 944, 93, 359367], 'precision': 0.7263768115942029, 'sensitivity': 0.9642170065409773, 'no_links': 3450, 'F-score': 0.8285667052405356}\n",
      "Fold 2 {'no_false': 1226, 'confusion_matrix': [2507, 1134, 92, 359177], 'precision': 0.6885471024443834, 'sensitivity': 0.9646017699115044, 'no_links': 3641, 'F-score': 0.8035256410256411}\n",
      "Fold 3 {'no_false': 1017, 'confusion_matrix': [2506, 924, 93, 359387], 'precision': 0.7306122448979592, 'sensitivity': 0.9642170065409773, 'no_links': 3430, 'F-score': 0.8313153093381987}\n",
      "Fold 4 {'no_false': 1099, 'confusion_matrix': [2506, 1006, 93, 359305], 'precision': 0.7135535307517085, 'sensitivity': 0.9642170065409773, 'no_links': 3512, 'F-score': 0.8201603665521192}\n",
      "Fold 5 {'no_false': 1004, 'confusion_matrix': [2506, 911, 93, 359400], 'precision': 0.7333918642083699, 'sensitivity': 0.9642170065409773, 'no_links': 3417, 'F-score': 0.8331117021276595}\n",
      "Fold 6 {'no_false': 1101, 'confusion_matrix': [2506, 1008, 93, 359303], 'precision': 0.7131474103585658, 'sensitivity': 0.9642170065409773, 'no_links': 3514, 'F-score': 0.8198920333715034}\n",
      "Fold 7 {'no_false': 1256, 'confusion_matrix': [2509, 1166, 90, 359145], 'precision': 0.6827210884353742, 'sensitivity': 0.9653712966525587, 'no_links': 3675, 'F-score': 0.7998087344596749}\n",
      "Fold 8 {'no_false': 1158, 'confusion_matrix': [2507, 1066, 92, 359245], 'precision': 0.7016512734396866, 'sensitivity': 0.9646017699115044, 'no_links': 3573, 'F-score': 0.8123784834737524}\n",
      "Fold 9 {'no_false': 1145, 'confusion_matrix': [2506, 1052, 93, 359259], 'precision': 0.7043282743114109, 'sensitivity': 0.9642170065409773, 'no_links': 3558, 'F-score': 0.8140328081858047}\n",
      "nn bagging: {'no_false': 1103, 'confusion_matrix': [2506, 1010, 93, 359301], 'precision': 0.7127417519908987, 'sensitivity': 0.9642170065409773, 'no_links': 3516, 'F-score': 0.8196238757154538}\n",
      "\n",
      "lg per fold:\n",
      "Fold 0 {'no_false': 1692, 'confusion_matrix': [2511, 1604, 88, 358707], 'precision': 0.6102065613608748, 'sensitivity': 0.966140823393613, 'no_links': 4115, 'F-score': 0.7479892761394102}\n",
      "Fold 1 {'no_false': 1718, 'confusion_matrix': [2510, 1629, 89, 358682], 'precision': 0.6064266731094468, 'sensitivity': 0.9657560600230858, 'no_links': 4139, 'F-score': 0.7450281982784208}\n",
      "Fold 2 {'no_false': 1744, 'confusion_matrix': [2516, 1661, 83, 358650], 'precision': 0.6023461814699546, 'sensitivity': 0.9680646402462486, 'no_links': 4177, 'F-score': 0.7426210153482881}\n",
      "Fold 3 {'no_false': 1695, 'confusion_matrix': [2511, 1607, 88, 358704], 'precision': 0.6097620203982516, 'sensitivity': 0.966140823393613, 'no_links': 4118, 'F-score': 0.7476552032157213}\n",
      "Fold 4 {'no_false': 1697, 'confusion_matrix': [2511, 1609, 88, 358702], 'precision': 0.6094660194174757, 'sensitivity': 0.966140823393613, 'no_links': 4120, 'F-score': 0.7474326536687007}\n",
      "Fold 5 {'no_false': 1677, 'confusion_matrix': [2510, 1588, 89, 358723], 'precision': 0.6124938994631528, 'sensitivity': 0.9657560600230858, 'no_links': 4098, 'F-score': 0.7495893683738989}\n",
      "Fold 6 {'no_false': 1698, 'confusion_matrix': [2511, 1610, 88, 358701], 'precision': 0.6093181266682844, 'sensitivity': 0.966140823393613, 'no_links': 4121, 'F-score': 0.7473214285714287}\n",
      "Fold 7 {'no_false': 1717, 'confusion_matrix': [2517, 1635, 82, 358676], 'precision': 0.6062138728323699, 'sensitivity': 0.9684494036167757, 'no_links': 4152, 'F-score': 0.745667308546882}\n",
      "Fold 8 {'no_false': 1678, 'confusion_matrix': [2511, 1590, 88, 358721], 'precision': 0.612289685442575, 'sensitivity': 0.966140823393613, 'no_links': 4101, 'F-score': 0.7495522388059702}\n",
      "Fold 9 {'no_false': 1710, 'confusion_matrix': [2511, 1622, 88, 358689], 'precision': 0.607548995886765, 'sensitivity': 0.966140823393613, 'no_links': 4133, 'F-score': 0.7459893048128341}\n",
      "lg bagging: {'no_false': 1694, 'confusion_matrix': [2511, 1606, 88, 358705], 'precision': 0.6099101287345154, 'sensitivity': 0.966140823393613, 'no_links': 4117, 'F-score': 0.7477665276950566}\n",
      "\n",
      "STACKING PERFORMANCE:\n",
      "\n",
      "{'no_false': 997, 'confusion_matrix': [2505, 903, 94, 359408], 'precision': 0.7350352112676056, 'sensitivity': 0.9638322431704501, 'no_links': 3408, 'F-score': 0.8340269685367071}\n"
     ]
    }
   ],
   "source": [
    "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
    "\n",
    "print(\"BAGGING PERFORMANCE:\\n\")\n",
    "modeltypes = ['svm', 'nn', 'lg'] \n",
    "modeltypes_2 = ['rbf', 'relu', 'l2']\n",
    "modelparams = [0.001, 2000, 0.005]\n",
    "nFold = 10\n",
    "kf = KFold(n_splits=nFold)\n",
    "model_raw_score = [0]*3\n",
    "model_binary_score = [0]*3\n",
    "model_i = 0\n",
    "for model_i in range(3):\n",
    "    modeltype = modeltypes[model_i]\n",
    "    modeltype_2 = modeltypes_2[model_i]\n",
    "    modelparam = modelparams[model_i]\n",
    "    print(modeltype, \"per fold:\")\n",
    "    iFold = 0\n",
    "    result_fold = [0]*nFold\n",
    "    final_eval_fold = [0]*nFold\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
    "        result_fold[iFold] = classify(md, X_test)\n",
    "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
    "        print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
    "        iFold = iFold + 1\n",
    "    bagging_raw_score = np.average(result_fold, axis=0)\n",
    "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
    "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
    "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
    "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
    "    print(modeltype, \"bagging:\", bagging_eval)\n",
    "    print('')\n",
    "    model_raw_score[model_i] = bagging_raw_score\n",
    "    model_binary_score[model_i] = bagging_binary_score\n",
    "    \n",
    "thres = .99\n",
    "print(\"STACKING PERFORMANCE:\\n\")\n",
    "stack_raw_score = np.average(model_raw_score, axis=0)\n",
    "stack_binary_score = np.copy(stack_raw_score)\n",
    "stack_binary_score[stack_binary_score > thres] = 1\n",
    "stack_binary_score[stack_binary_score <= thres] = 0\n",
    "stacking_eval = evaluation(y_test, stack_binary_score)\n",
    "print(stacking_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS598 Project Code\n",
    "\n",
    "The following contain our own code for replicating and validating the results of the original paper. The data sets and therefore data preprocessing constructs from the original paper are reused here rather than being reimplemented. Note that the implementation of both the paper's and our models are identical for both Schemes A and B; only hyperparameters differ.\n",
    "\n",
    "The code is organized into sections:\n",
    "1. Base Learner Bagging\n",
    "     1. Support Vector Machine\n",
    "     1. Neural Network\n",
    "     1. Linear Regression\n",
    "1. Stacking\n",
    "\n",
    "Following our code is an \"appendix\" section which performs out-of-context validation on the reference implementation's base learners' performance using the same parameterization and data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the torch library which we'll use for our implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy training and testing sets to torch.tensor for us with the PyTorch library\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Support Vector Machine Base Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1 SVM Model Implementation\n",
    "\n",
    "The support vector machine implementation, FEBRLReproducerSVM, reproduces the results of the support vector machine base learner from the original paper. This model has two initialization parameters:\n",
    "1. `num_features`: The number of features in the dataset; for the base dataset, this value is 13, but this value can differ if a dataset with fewer or additional features to be used.\n",
    "1. `inverse_reg`: The hyperparameter that the paper explored via grid search to determine optimal inverse regularization strength for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Support Vector Machine / Implementation\n",
    "\n",
    "# For SVM, shift our data from its native range of [0.0, 1.0] to [-1.0, 1.0]\n",
    "X_train_tensor_svm = (X_train_tensor * 2) - 1\n",
    "y_train_tensor_svm = (y_train_tensor * 2) - 1\n",
    "X_test_tensor_svm = (X_test_tensor * 2) - 1\n",
    "y_test_tensor_svm = (y_test_tensor * 2) - 1\n",
    "\n",
    "class FEBRLReproducerSVM(nn.Module):\n",
    "    def __init__(self, num_features, inverse_reg=0.0):\n",
    "        # Create the our PyTorch support vector machine model\n",
    "        super(FEBRLReproducerSVM, self).__init__()\n",
    "\n",
    "        # STEP 1\n",
    "        # Specify parameters for our PyTorch nn model based upon the analogous\n",
    "        # parameters used by the original paper's sklearn SVC\n",
    "\n",
    "        # PyTorch SVM (nn) concept                       Analogous sklearn SVC parameter\n",
    "        # ------------------------                      -------------------------------\n",
    "        self.inverse_reg = inverse_reg                  # C (inverse of the regularization strength)\n",
    "        #                                               # kernel (original paper uses linear, as do we)\n",
    "\n",
    "        # STEP 2\n",
    "        # Define the layers for our lr model\n",
    "        self.num_input_features = num_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.num_input_features, out_features=1, bias=False)\n",
    "\n",
    "        # STEP 3\n",
    "        # Define the criteria and optimizer\n",
    "        self.num_max_epochs = 5000\n",
    "        self.criterion = nn.HingeEmbeddingLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "            lr = 0.001,\n",
    "            weight_decay=inverse_reg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform a forward pass on the nn; it is not recommended to call this\n",
    "        # function directly, but to instead call fit(...) or predict(...) so that model's\n",
    "        # mode is correctly set automatically\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Train the nn with the specified parameters; analogous to sklearn's\n",
    "        # SVC.fit(...) method\n",
    "        self.train()\n",
    "\n",
    "        loss_previous_epoch = 1.0\n",
    "        loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "        for epoch_i in np.arange(self.num_max_epochs):\n",
    "            loss = None\n",
    "            kfold = KFold(n_splits=10, shuffle=True, random_state=12345)\n",
    "\n",
    "            for train_indicies, _ in kfold.split(X_train):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(X_train[train_indicies])\n",
    "                output *= -1\n",
    "                loss = self.criterion(output, y_train[train_indicies])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Determine if criteria for early training termination is satisfied\n",
    "            if (np.abs(loss_previous_epoch - loss.item())) <= 0.0001:\n",
    "                loss_consecutive_epochs_minimal = loss_consecutive_epochs_minimal + 1\n",
    "\n",
    "                if(loss_consecutive_epochs_minimal == 50):\n",
    "                    break\n",
    "            else:\n",
    "                loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "            loss_previous_epoch = loss.item()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Test the nn with the specified parameters; analogous to sklearn's\n",
    "        # SVC.predict(...) method\n",
    "        self.eval()\n",
    "        return self.forward(X_test)\n",
    "\n",
    "frs_inverse_reg_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000] \n",
    "frs_inverse_reg_optimal = 1000  # Determined through search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2. Support Vector Machine Hyperparameter Search\n",
    "\n",
    "Grid search is performed on the set of candidate hyperparameters from the original paper using our model. Some hyperparameters may take a long time to test if they do not cause the model's loss to converge early (or at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay = 0.001: {'no_false': 358153, 'confusion_matrix': [2597, 358151, 2, 2160], 'precision': 0.007198931109805183, 'sensitivity': 0.9992304732589458, 'no_links': 360748, 'F-score': 0.014294875146898145}\n",
      "weight_decay = 0.002: {'no_false': 358202, 'confusion_matrix': [2597, 358200, 2, 2111], 'precision': 0.007197953419790076, 'sensitivity': 0.9992304732589458, 'no_links': 360797, 'F-score': 0.014292947638388976}\n",
      "weight_decay = 0.005: {'no_false': 358167, 'confusion_matrix': [2597, 358165, 2, 2146], 'precision': 0.007198651742700173, 'sensitivity': 0.9992304732589458, 'no_links': 360762, 'F-score': 0.014294324377134585}\n",
      "weight_decay = 0.01: {'no_false': 358147, 'confusion_matrix': [2597, 358145, 2, 2166], 'precision': 0.007199050845202388, 'sensitivity': 0.9992304732589458, 'no_links': 360742, 'F-score': 0.014295111204075511}\n",
      "weight_decay = 0.02: {'no_false': 358154, 'confusion_matrix': [2597, 358152, 2, 2159], 'precision': 0.007198911154292874, 'sensitivity': 0.9992304732589458, 'no_links': 360749, 'F-score': 0.014294835804793201}\n",
      "weight_decay = 0.05: {'no_false': 357990, 'confusion_matrix': [2597, 357988, 2, 2323], 'precision': 0.007202185337715102, 'sensitivity': 0.9992304732589458, 'no_links': 360585, 'F-score': 0.01430129080576237}\n",
      "weight_decay = 0.1: {'no_false': 357447, 'confusion_matrix': [2597, 357445, 2, 2866], 'precision': 0.007213047366696108, 'sensitivity': 0.9992304732589458, 'no_links': 360042, 'F-score': 0.01432270482377889}\n",
      "weight_decay = 0.2: {'no_false': 352975, 'confusion_matrix': [2597, 352973, 2, 7338], 'precision': 0.007303765784515004, 'sensitivity': 0.9992304732589458, 'no_links': 355570, 'F-score': 0.014501534191959662}\n",
      "weight_decay = 0.5: {'no_false': 301583, 'confusion_matrix': [2593, 301577, 6, 58734], 'precision': 0.008524838083966204, 'sensitivity': 0.9976914197768373, 'no_links': 304170, 'F-score': 0.016905228364013314}\n",
      "weight_decay = 1: {'no_false': 117355, 'confusion_matrix': [2581, 117337, 18, 242974], 'precision': 0.021523040744508747, 'sensitivity': 0.9930742593305117, 'no_links': 119918, 'F-score': 0.04213292849155627}\n",
      "weight_decay = 5: {'no_false': 350, 'confusion_matrix': [2490, 241, 109, 360070], 'precision': 0.9117539362870744, 'sensitivity': 0.9580607926125433, 'no_links': 2731, 'F-score': 0.9343339587242026}\n",
      "weight_decay = 10: {'no_false': 312, 'confusion_matrix': [2488, 201, 111, 360110], 'precision': 0.925251022685013, 'sensitivity': 0.957291265871489, 'no_links': 2689, 'F-score': 0.9409984871406959}\n",
      "weight_decay = 20: {'no_false': 316, 'confusion_matrix': [2488, 205, 111, 360106], 'precision': 0.9238767174155217, 'sensitivity': 0.957291265871489, 'no_links': 2693, 'F-score': 0.9402872260015117}\n",
      "weight_decay = 50: {'no_false': 318, 'confusion_matrix': [2488, 207, 111, 360104], 'precision': 0.9231910946196661, 'sensitivity': 0.957291265871489, 'no_links': 2695, 'F-score': 0.9399319984888553}\n",
      "weight_decay = 100: {'no_false': 320, 'confusion_matrix': [2488, 209, 111, 360102], 'precision': 0.9225064886911383, 'sensitivity': 0.957291265871489, 'no_links': 2697, 'F-score': 0.9395770392749244}\n",
      "weight_decay = 200: {'no_false': 324, 'confusion_matrix': [2488, 213, 111, 360098], 'precision': 0.9211403184005924, 'sensitivity': 0.957291265871489, 'no_links': 2701, 'F-score': 0.9388679245283019}\n",
      "weight_decay = 500: {'no_false': 319, 'confusion_matrix': [2488, 208, 111, 360103], 'precision': 0.9228486646884273, 'sensitivity': 0.957291265871489, 'no_links': 2696, 'F-score': 0.9397544853635506}\n",
      "weight_decay = 1000: {'no_false': 289, 'confusion_matrix': [2487, 177, 112, 360134], 'precision': 0.9335585585585585, 'sensitivity': 0.9569065025009619, 'no_links': 2664, 'F-score': 0.9450883526505794}\n",
      "weight_decay = 2000: {'no_false': 274167, 'confusion_matrix': [2213, 273781, 386, 86530], 'precision': 0.008018290252686652, 'sensitivity': 0.8514813389765294, 'no_links': 275994, 'F-score': 0.015886974906045737}\n",
      "weight_decay = 5000: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Temp\\ipykernel_1956\\4019262146.py:145: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = count_true_pos/(count_true_pos+count_false_pos)\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Support Vector Machine / Hyperparameter Search\n",
    "\n",
    "# Perform SVM base learner evaluation using the hyperparameter search range provided by the original paper\n",
    "for inverse_reg in frs_inverse_reg_range:\n",
    "    # Create an instance of the SVM\n",
    "    febrl_reproducer_svm = FEBRLReproducerSVM(num_features=X_train_tensor.shape[1], inverse_reg=inverse_reg)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_svm.fit(X_train_tensor_svm, y_train_tensor_svm)\n",
    "\n",
    "    # Test the model\n",
    "    frs_output = febrl_reproducer_svm.predict(X_test_tensor_svm).detach()\n",
    "\n",
    "    y_pred = np.asarray([1 if element > 0 else 0 for element in frs_output])\n",
    "\n",
    "    print(\"weight_decay = {}: {}\".format(inverse_reg, evaluation(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3. Support Vector Machine Training, Evaluation, and Bagging\n",
    "\n",
    "The reference implementation uses a 10-split k-fold as their bootstrapping technique. We will do the same here to ensure that our implementation is trained with the same data as the reference implementation. After each base learner is trained, it is evaluated with the test data set. After all base learners have evaluated the test data set, their outputs are passed through the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 0: {'no_false': 271, 'confusion_matrix': [2486, 158, 113, 360153], 'precision': 0.9402420574886535, 'sensitivity': 0.9565217391304348, 'no_links': 2644, 'F-score': 0.9483120350944115}\n",
      "Execution 1: {'no_false': 260, 'confusion_matrix': [2486, 147, 113, 360164], 'precision': 0.9441701481200152, 'sensitivity': 0.9565217391304348, 'no_links': 2633, 'F-score': 0.9503058103975535}\n",
      "Execution 2: {'no_false': 337, 'confusion_matrix': [2489, 227, 110, 360084], 'precision': 0.916421207658321, 'sensitivity': 0.9576760292420161, 'no_links': 2716, 'F-score': 0.9365945437441204}\n",
      "Execution 3: {'no_false': 355, 'confusion_matrix': [2490, 246, 109, 360065], 'precision': 0.9100877192982456, 'sensitivity': 0.9580607926125433, 'no_links': 2736, 'F-score': 0.9334582942830365}\n",
      "Execution 4: {'no_false': 270, 'confusion_matrix': [2486, 157, 113, 360154], 'precision': 0.9405978055240257, 'sensitivity': 0.9565217391304348, 'no_links': 2643, 'F-score': 0.9484929416253338}\n",
      "Execution 5: {'no_false': 320, 'confusion_matrix': [2488, 209, 111, 360102], 'precision': 0.9225064886911383, 'sensitivity': 0.957291265871489, 'no_links': 2697, 'F-score': 0.9395770392749244}\n",
      "Execution 6: {'no_false': 276, 'confusion_matrix': [2486, 163, 113, 360148], 'precision': 0.9384673461683655, 'sensitivity': 0.9565217391304348, 'no_links': 2649, 'F-score': 0.9474085365853658}\n",
      "Execution 7: {'no_false': 331, 'confusion_matrix': [2488, 220, 111, 360091], 'precision': 0.9187592319054653, 'sensitivity': 0.957291265871489, 'no_links': 2708, 'F-score': 0.9376295458827963}\n",
      "Execution 8: {'no_false': 322, 'confusion_matrix': [2488, 211, 111, 360100], 'precision': 0.921822897369396, 'sensitivity': 0.957291265871489, 'no_links': 2699, 'F-score': 0.93922234805587}\n",
      "Execution 9: {'no_false': 324, 'confusion_matrix': [2488, 213, 111, 360098], 'precision': 0.9211403184005924, 'sensitivity': 0.957291265871489, 'no_links': 2701, 'F-score': 0.9388679245283019}\n",
      "SVM bagging: {'no_false': 305, 'confusion_matrix': [2488, 194, 111, 360117], 'precision': 0.9276659209545116, 'sensitivity': 0.957291265871489, 'no_links': 2682, 'F-score': 0.9422457867828063}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Support Vector Machine / Training, Evaluation, and Bagging\n",
    "\n",
    "# Perform bagging across 10 models\n",
    "frs_kfold_count = 10\n",
    "frs_kfold = KFold(n_splits=frs_kfold_count, shuffle=True, random_state=12345)\n",
    "frs_kfold_i = 0\n",
    "\n",
    "frs_results = [0] * frs_kfold_count\n",
    "\n",
    "for train_indicies, _ in frs_kfold.split(X_train):\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_svm = FEBRLReproducerSVM(num_features=X_train_tensor.shape[1], inverse_reg=frs_inverse_reg_optimal)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_svm.fit(X_train_tensor_svm[train_indicies], y_train_tensor_svm[train_indicies])\n",
    "\n",
    "    # Test the model\n",
    "    frs_results[frs_kfold_i] = febrl_reproducer_svm.predict(X_test_tensor_svm).detach().numpy()\n",
    "\n",
    "    # Print the results of the current base learner for convenience\n",
    "    y_pred = np.asarray([1 if element > 0 else 0 for element in frs_results[frs_kfold_i]])\n",
    "    print(\"Execution {}: {}\".format(frs_kfold_i, evaluation(y_test, y_pred)))\n",
    "\n",
    "    frs_kfold_i = frs_kfold_i + 1\n",
    "\n",
    "frs_bagging_raw_score = np.average(frs_results, axis=0)\n",
    "frs_bagging_binary_score = np.copy(frs_bagging_raw_score)\n",
    "frs_bagging_binary_score[frs_bagging_binary_score > 0] = 1\n",
    "frs_bagging_binary_score[frs_bagging_binary_score <= 0] = 0\n",
    "frs_bagging_evaluation = evaluation(y_test, frs_bagging_binary_score)\n",
    "print(\"SVM bagging: {}\".format(frs_bagging_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Neural Network Base Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1. Neural Network Model Implementation\n",
    "\n",
    "The neural network implementation, FEBRLReproducerNN, reproduces the results of the neural network base learner from the original paper. This model has two initialization parameters:\n",
    "1. `num_features`: The number of features in the dataset; for the base dataset, this value is 13, but this value can differ if a dataset with fewer or additional features to be used.\n",
    "1. `weight_decay`: The hyperparameter that the paper explored via grid search to determine optimal weight decay for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Neural Network Model / Implementation\n",
    "\n",
    "class FEBRLReproducerNN(nn.Module):\n",
    "    def __init__(self, num_features, weight_decay=0.0):\n",
    "        # Create the our PyTorch nn model\n",
    "        super(FEBRLReproducerNN, self).__init__()\n",
    "\n",
    "        # STEP 1\n",
    "        # Specify parameters for our PyTorch nn model based upon the analogous\n",
    "        # parameters used by the original paper's sklearn MLPClassifier\n",
    "\n",
    "        # PyTorch nn concept                            Analogous sklearn MLPClassifier parameter\n",
    "        # ------------------                            -----------------------------------------\n",
    "        self.optimizer = None                           # solver (optimizer; original paper uses LBFGS, but we will use SGD (defined later) due to PyTorch-sklearn differences)\n",
    "        self.optimizer_weight_decay = weight_decay      # alpha (L2 penalty/regularization term)\n",
    "        self.num_hidden_layer_nodes = 256               # hidden_layer_sizes (tuple of hidden layer nodes)\n",
    "        self.activation = F.relu                        # activation (activation function)\n",
    "        self.random_state = 12345                       # random_state (static, random state for reproducibility)\n",
    "        #                                               # batch_size (minibatch size; unused in our model)\n",
    "        #                                               # learning_rate (tells the model to use the provided initial learning rate; n/a to our model)\n",
    "        self.optimizer_learning_rate_init = 0.001       # learning_rate_init (initial learning rate)\n",
    "        self.optimizer_dampening = 0.5                  # power_t (dampening)\n",
    "        self.num_max_epochs = 30000                     # max_iter (maximum number of epochs when using stochastic optimizers)\n",
    "        self.shuffle = True                             # shuffle (shuffle samples in each iteration)\n",
    "        self.tolerance = 0.0001                         # tol (optimization tolorance; early training termination)\n",
    "        #                                               # verbose (print model progress debug messages to console; specified by unused by original paper)\n",
    "        #                                               # warm_start (initialize the model with the results of previous executions; specified but unused by original paper)\n",
    "        self.optimizer_momentum = 0.9                   # momentum (optimizer momentum)\n",
    "        self.use_nesterov_momentum = True               # nesterovs_momentum (use Nesterov's momentum in the optimizer)\n",
    "        #                                               # early_stopping (terminate early when validation is not improving; 'False' in original paper)\n",
    "        #                                               # validation_fraction (validation data set criteria for early stopping; specified by unused by original paper)\n",
    "        #                                               # beta_1 (parameter for Adam optimizer; specified but unused by original paper)\n",
    "        #                                               # beta_2 (parameter for Adam optimizer; specified but unused by original paper)\n",
    "        #                                               # epsilon (parameter for Adam optimizer; specified but unused by original paper)\n",
    "\n",
    "        # STEP 2\n",
    "        # Define the layers for our nn model\n",
    "        self.num_input_features = num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.num_input_features, out_features=self.num_hidden_layer_nodes, bias=False)\n",
    "        self.fc2 = nn.Linear(in_features=self.num_hidden_layer_nodes, out_features=1, bias=False)\n",
    "\n",
    "        # STEP 3\n",
    "        # Define the criteria and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "            lr=self.optimizer_learning_rate_init,\n",
    "            weight_decay=self.optimizer_weight_decay,\n",
    "            momentum=self.optimizer_momentum,\n",
    "            dampening=0,\n",
    "            nesterov=self.use_nesterov_momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform a forward pass on the nn; it is not recommended to call this\n",
    "        # function directly, but to instead call fit(...) or predict(...) so that model's\n",
    "        # mode is correctly set automatically\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Train the nn with the specified parameters; analogous to sklearn's\n",
    "        # MLPClassifier.fit(...) method\n",
    "        self.train()\n",
    "    \n",
    "        loss_previous_epoch = 1.0\n",
    "        loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "        for epoch_i in np.arange(self.num_max_epochs):\n",
    "            loss = None\n",
    "            kfold = KFold(n_splits=10, shuffle=self.shuffle, random_state=self.random_state)\n",
    "\n",
    "            for train_indicies, _ in kfold.split(X_train):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(X_train[train_indicies])\n",
    "                loss = self.criterion(output, y_train[train_indicies])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Determine if criteria for early training termination is satisfied\n",
    "            if (loss_previous_epoch - loss.item()) <= self.tolerance:\n",
    "                loss_consecutive_epochs_minimal = loss_consecutive_epochs_minimal + 1\n",
    "\n",
    "                if(loss_consecutive_epochs_minimal == 50):\n",
    "                    break\n",
    "            else:\n",
    "                loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "            loss_previous_epoch = loss.item()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Test the nn with the specified parameters; analogous to sklearn's\n",
    "        # MLPClassifier.predict(...) method\n",
    "        self.eval()\n",
    "        return self.forward(X_test)\n",
    "\n",
    "frn_weight_decay_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000]\n",
    "frn_weight_decay_optimal = 0.5  # Determined through search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Neural Network Hyperparameter Search\n",
    "\n",
    "Grid search is performed on the set of candidate hyperparameters from the original paper using our model. Some hyperparameters may take a long time to test if they do not cause the model's loss to converge early (or at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay = 0.001: {'no_false': 67060, 'confusion_matrix': [2581, 67042, 18, 293269], 'precision': 0.03707108283182282, 'sensitivity': 0.9930742593305117, 'no_links': 69623, 'F-score': 0.07147406607404946}\n",
      "weight_decay = 0.002: {'no_false': 67160, 'confusion_matrix': [2579, 67140, 20, 293171], 'precision': 0.03699135099470732, 'sensitivity': 0.9923047325894575, 'no_links': 69719, 'F-score': 0.07132387510716556}\n",
      "weight_decay = 0.005: {'no_false': 77438, 'confusion_matrix': [2580, 77419, 19, 282892], 'precision': 0.03225040313003912, 'sensitivity': 0.9926894959599846, 'no_links': 79999, 'F-score': 0.062471246277149575}\n",
      "weight_decay = 0.01: {'no_false': 70490, 'confusion_matrix': [2582, 70473, 17, 289838], 'precision': 0.03534323454931216, 'sensitivity': 0.9934590227010388, 'no_links': 73055, 'F-score': 0.06825812250508895}\n",
      "weight_decay = 0.02: {'no_false': 70099, 'confusion_matrix': [2580, 70080, 19, 290231], 'precision': 0.03550784475639967, 'sensitivity': 0.9926894959599846, 'no_links': 72660, 'F-score': 0.06856322831820778}\n",
      "weight_decay = 0.05: {'no_false': 5962, 'confusion_matrix': [2560, 5923, 39, 354388], 'precision': 0.30178003064953435, 'sensitivity': 0.9849942285494421, 'no_links': 8483, 'F-score': 0.4620104674246526}\n",
      "weight_decay = 0.1: {'no_false': 1834, 'confusion_matrix': [2521, 1756, 78, 358555], 'precision': 0.5894318447509936, 'sensitivity': 0.9699884570988841, 'no_links': 4277, 'F-score': 0.7332751599767305}\n",
      "weight_decay = 0.2: {'no_false': 1248, 'confusion_matrix': [2502, 1151, 97, 359160], 'precision': 0.6849165069805639, 'sensitivity': 0.9626779530588688, 'no_links': 3653, 'F-score': 0.800383877159309}\n",
      "weight_decay = 0.5: {'no_false': 283, 'confusion_matrix': [2484, 168, 115, 360143], 'precision': 0.9366515837104072, 'sensitivity': 0.9557522123893806, 'no_links': 2652, 'F-score': 0.9461055037135785}\n",
      "weight_decay = 1: {'no_false': 1020, 'confusion_matrix': [1589, 10, 1010, 360301], 'precision': 0.9937460913070669, 'sensitivity': 0.611388995767603, 'no_links': 1599, 'F-score': 0.7570271557884707}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Temp\\ipykernel_1956\\4019262146.py:145: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = count_true_pos/(count_true_pos+count_false_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay = 5: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 10: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 20: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 50: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 100: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 200: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 500: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "weight_decay = 1000: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\repos\\Medical-Record-Linkage-Ensemble\\ePBRN_UNSW_Linkage.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000036?line=5'>6</a>\u001b[0m febrl_reproducer_nn \u001b[39m=\u001b[39m FEBRLReproducerNN(num_features\u001b[39m=\u001b[39mX_train_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000036?line=7'>8</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000036?line=8'>9</a>\u001b[0m febrl_reproducer_nn\u001b[39m.\u001b[39;49mfit(X_train_tensor, y_train_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000036?line=10'>11</a>\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000036?line=11'>12</a>\u001b[0m frn_output \u001b[39m=\u001b[39m febrl_reproducer_nn\u001b[39m.\u001b[39mpredict(X_test_tensor)\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[1;32md:\\repos\\Medical-Record-Linkage-Ensemble\\ePBRN_UNSW_Linkage.ipynb Cell 27'\u001b[0m in \u001b[0;36mFEBRLReproducerNN.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000034?line=75'>76</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X_train[train_indicies])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000034?line=76'>77</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, y_train[train_indicies])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000034?line=77'>78</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000034?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/ePBRN_UNSW_Linkage.ipynb#ch0000034?line=80'>81</a>\u001b[0m \u001b[39m# Determine if criteria for early training termination is satisfied\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Neural Network Model / Hyperparameter Search\n",
    "\n",
    "# Perform nn base learner evaluation using the hyperparameter search range provided by the original paper\n",
    "for weight_decay in frn_weight_decay_range:\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_nn = FEBRLReproducerNN(num_features=X_train_tensor.shape[1], weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_nn.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Test the model\n",
    "    frn_output = febrl_reproducer_nn.predict(X_test_tensor).detach()\n",
    "\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frn_output])\n",
    "\n",
    "    print(\"weight_decay = {}: {}\".format(weight_decay, evaluation(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3. Neural Network Training, Evaluation, and Bagging\n",
    "\n",
    "The reference implementation uses a 10-split k-fold as their bootstrapping technique. We will do the same here to ensure that our implementation is trained with the same data as the reference implementation. After each base learner is trained, it is evaluated with the test data set. After all base learners have evaluated the test data set, their outputs are passed through the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 0: {'no_false': 327, 'confusion_matrix': [2486, 214, 113, 360097], 'precision': 0.9207407407407407, 'sensitivity': 0.9565217391304348, 'no_links': 2700, 'F-score': 0.9382902434421589}\n",
      "Execution 1: {'no_false': 281, 'confusion_matrix': [2483, 165, 116, 360146], 'precision': 0.9376888217522659, 'sensitivity': 0.9553674490188534, 'no_links': 2648, 'F-score': 0.9464455879550219}\n",
      "Execution 2: {'no_false': 312, 'confusion_matrix': [2487, 200, 112, 360111], 'precision': 0.9255675474506885, 'sensitivity': 0.9569065025009619, 'no_links': 2687, 'F-score': 0.9409761634506242}\n",
      "Execution 3: {'no_false': 333, 'confusion_matrix': [2486, 220, 113, 360091], 'precision': 0.9186991869918699, 'sensitivity': 0.9565217391304348, 'no_links': 2706, 'F-score': 0.9372290292177192}\n",
      "Execution 4: {'no_false': 296, 'confusion_matrix': [2484, 181, 115, 360130], 'precision': 0.9320825515947467, 'sensitivity': 0.9557522123893806, 'no_links': 2665, 'F-score': 0.9437689969604864}\n",
      "Execution 5: {'no_false': 356, 'confusion_matrix': [2487, 244, 112, 360067], 'precision': 0.9106554375686562, 'sensitivity': 0.9569065025009619, 'no_links': 2731, 'F-score': 0.9332082551594748}\n",
      "Execution 6: {'no_false': 279, 'confusion_matrix': [2482, 162, 117, 360149], 'precision': 0.9387291981845688, 'sensitivity': 0.9549826856483262, 'no_links': 2644, 'F-score': 0.9467861911119587}\n",
      "Execution 7: {'no_false': 357, 'confusion_matrix': [2488, 246, 111, 360065], 'precision': 0.9100219458668617, 'sensitivity': 0.957291265871489, 'no_links': 2734, 'F-score': 0.9330583161447591}\n",
      "Execution 8: {'no_false': 324, 'confusion_matrix': [2486, 211, 113, 360100], 'precision': 0.9217649239896181, 'sensitivity': 0.9565217391304348, 'no_links': 2697, 'F-score': 0.9388217522658611}\n",
      "Execution 9: {'no_false': 349, 'confusion_matrix': [2486, 236, 113, 360075], 'precision': 0.9132990448199853, 'sensitivity': 0.9565217391304348, 'no_links': 2722, 'F-score': 0.9344108250328885}\n",
      "Neural Network bagging: {'no_false': 323, 'confusion_matrix': [2486, 210, 113, 360101], 'precision': 0.922106824925816, 'sensitivity': 0.9565217391304348, 'no_links': 2696, 'F-score': 0.9389990557129367}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Neural Network Model / Training, Evaluation, and Bagging\n",
    "\n",
    "# Perform bagging across 10 models\n",
    "frn_kfold_count = 10\n",
    "frn_kfold = KFold(n_splits=frn_kfold_count, shuffle=True, random_state=12345)\n",
    "frn_kfold_i = 0\n",
    "\n",
    "frn_results = [0] * frn_kfold_count\n",
    "\n",
    "for train_indicies, _ in frn_kfold.split(X_train):\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_nn = FEBRLReproducerNN(num_features=X_train_tensor.shape[1], weight_decay=frn_weight_decay_optimal)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_nn.fit(X_train_tensor[train_indicies], y_train_tensor[train_indicies])\n",
    "\n",
    "    # Test the model\n",
    "    frn_results[frn_kfold_i] = febrl_reproducer_nn.predict(X_test_tensor).detach().numpy()\n",
    "\n",
    "    # Print the results of the current base learner for convenience\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frn_results[frn_kfold_i]])\n",
    "    print(\"Execution {}: {}\".format(frn_kfold_i, evaluation(y_test, y_pred)))\n",
    "\n",
    "    frn_kfold_i = frn_kfold_i + 1\n",
    "\n",
    "frn_bagging_raw_score = np.average(frn_results, axis=0)\n",
    "frn_bagging_binary_score = np.copy(frn_bagging_raw_score)\n",
    "frn_bagging_binary_score[frn_bagging_binary_score > 0.5] = 1\n",
    "frn_bagging_binary_score[frn_bagging_binary_score <= 0.5] = 0\n",
    "frn_bagging_evaluation = evaluation(y_test, frn_bagging_binary_score)\n",
    "print(\"Neural Network bagging: {}\".format(frn_bagging_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Logistic Regression Base Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1 Logistic Regression Model Implementation\n",
    "\n",
    "The logistic regression implementation, FEBRLReproducerLR, reproduces the results of the logistic regression base learner from the original paper. This model has two initialization parameters:\n",
    "1. `num_features`: The number of features in the dataset; for the base dataset, this value is 13, but this value can differ if a dataset with fewer or additional features to be used.\n",
    "1. `inverse_reg`: The hyperparameter that the paper explored via grid search to determine optimal inverse regularization strength for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Logistic Regression / Implementation\n",
    "\n",
    "class FEBRLReproducerLR(nn.Module):\n",
    "    def __init__(self, num_features, inverse_reg=0.0):\n",
    "        # Create the our PyTorch logistic regression model\n",
    "        super(FEBRLReproducerLR, self).__init__()\n",
    "\n",
    "        # STEP 1\n",
    "        # Specify parameters for our PyTorch LR model based upon the analogous\n",
    "        # parameters used by the original paper's sklearn LogisticRegression\n",
    "\n",
    "        # PyTorch LR (nn) concept                       Analogous sklearn LogisticRegression parameter\n",
    "        # -----------------------                       ----------------------------------------------\n",
    "        self.inverse_reg = inverse_reg                  # C (inverse of the regularization strength)\n",
    "        #                                               # penalty (original paper uses L2)\n",
    "        #                                               # dual (specifies dual formulation; specified but unused by the original paper)\n",
    "        self.use_bias = True                            # fit_intercept (specified if bias should be added to decision function; original paper specified this as true)\n",
    "        #                                               # intercept_scaling (intercept scaling, the original paper specifies this as 1)\n",
    "        self.num_max_epochs = 5000                      # max_iter (maximum number of epochs when using stochastic optimizers)\n",
    "        #                                               # multi_class (specifies class of problem; ours is a binary classification problem)\n",
    "        #                                               # n_jobs (the number of CPU cores used for parallelization; specified but unused by the original paper)\n",
    "        self.random_state = 12345                       # random_state (static, random state for reproducibility)\n",
    "\n",
    "        # STEP 2\n",
    "        # Define the layers for our lr model\n",
    "        self.num_input_features = num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.num_input_features, out_features=1, bias=self.use_bias)\n",
    "\n",
    "        # STEP 3\n",
    "        # Define the criteria and optimizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "            lr = 0.01,\n",
    "            weight_decay=self.inverse_reg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform a forward pass on the nn; it is not recommended to call this\n",
    "        # function directly, but to instead call fit(...) or predict(...) so that model's\n",
    "        # mode is correctly set automatically\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Train the nn with the specified parameters; analogous to sklearn's\n",
    "        # LogisticRegression.fit(...) method\n",
    "        self.train()\n",
    "\n",
    "        loss_previous_epoch = 1.0\n",
    "        loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "        for epoch_i in np.arange(self.num_max_epochs):\n",
    "            loss = None\n",
    "            kfold = KFold(n_splits=10, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "            for train_indicies, _ in kfold.split(X_train):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(X_train[train_indicies])\n",
    "                loss = self.criterion(output, y_train[train_indicies])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Determine if criteria for early training termination is satisfied\n",
    "            if (loss_previous_epoch - loss.item()) <= 0.0001:\n",
    "                loss_consecutive_epochs_minimal = loss_consecutive_epochs_minimal + 1\n",
    "\n",
    "                if(loss_consecutive_epochs_minimal == 50):\n",
    "                    break\n",
    "            else:\n",
    "                loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "            loss_previous_epoch = loss.item()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Test the nn with the specified parameters; analogous to sklearn's\n",
    "        # LogisticRegression.predict(...) method\n",
    "        self.eval()\n",
    "        return self.forward(X_test)\n",
    "\n",
    "frl_inverse_reg_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000] \n",
    "frl_inverse_reg_optimal = 0.5  # Determined through search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2 Logistic Regression Hyperparameter Search\n",
    "\n",
    "Grid search is performed on the set of candidate hyperparameters from the original paper using our model. Some hyperparameters may take a long time to test if they do not cause the model's loss to converge early (or at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse_reg = 0.001: {'no_false': 1331, 'confusion_matrix': [2503, 1235, 96, 359076], 'precision': 0.6696094168004281, 'sensitivity': 0.9630627164293959, 'no_links': 3738, 'F-score': 0.7899637052232918}\n",
      "inverse_reg = 0.002: {'no_false': 1413, 'confusion_matrix': [2507, 1321, 92, 358990], 'precision': 0.6549111807732497, 'sensitivity': 0.9646017699115044, 'no_links': 3828, 'F-score': 0.7801462579741714}\n",
      "inverse_reg = 0.005: {'no_false': 1277, 'confusion_matrix': [2502, 1180, 97, 359131], 'precision': 0.6795219989136339, 'sensitivity': 0.9626779530588688, 'no_links': 3682, 'F-score': 0.7966884254099664}\n",
      "inverse_reg = 0.01: {'no_false': 1216, 'confusion_matrix': [2498, 1115, 101, 359196], 'precision': 0.6913921948519236, 'sensitivity': 0.9611388995767602, 'no_links': 3613, 'F-score': 0.8042498390212492}\n",
      "inverse_reg = 0.02: {'no_false': 1209, 'confusion_matrix': [2499, 1109, 100, 359202], 'precision': 0.6926274944567627, 'sensitivity': 0.9615236629472874, 'no_links': 3608, 'F-score': 0.80521991300145}\n",
      "inverse_reg = 0.05: {'no_false': 1213, 'confusion_matrix': [2497, 1111, 102, 359200], 'precision': 0.6920731707317073, 'sensitivity': 0.9607541362062332, 'no_links': 3608, 'F-score': 0.8045754792975672}\n",
      "inverse_reg = 0.1: {'no_false': 1219, 'confusion_matrix': [2487, 1107, 112, 359204], 'precision': 0.6919866444073456, 'sensitivity': 0.9569065025009619, 'no_links': 3594, 'F-score': 0.8031648635556273}\n",
      "inverse_reg = 0.2: {'no_false': 978, 'confusion_matrix': [2486, 865, 113, 359446], 'precision': 0.741868099074903, 'sensitivity': 0.9565217391304348, 'no_links': 3351, 'F-score': 0.8356302521008404}\n",
      "inverse_reg = 0.5: {'no_false': 247, 'confusion_matrix': [2392, 40, 207, 360271], 'precision': 0.9835526315789473, 'sensitivity': 0.9203539823008849, 'no_links': 2432, 'F-score': 0.9509043927648578}\n",
      "inverse_reg = 1: {'no_false': 542, 'confusion_matrix': [2071, 14, 528, 360297], 'precision': 0.9932853717026379, 'sensitivity': 0.7968449403616775, 'no_links': 2085, 'F-score': 0.8842869342442358}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Temp\\ipykernel_1956\\4019262146.py:145: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = count_true_pos/(count_true_pos+count_false_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse_reg = 5: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 10: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 20: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 50: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 100: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 200: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 500: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 1000: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 2000: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n",
      "inverse_reg = 5000: {'no_false': 2599, 'confusion_matrix': [0, 0, 2599, 360311], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Logistic Regression Model / Hyperparameter Search\n",
    "\n",
    "# Perform logistic regression base learner evaluation using the hyperparameter search range provided by the original paper\n",
    "for inverse_reg in frl_inverse_reg_range:\n",
    "    # Create an instance of the logistic regression model\n",
    "    febrl_reproducer_lr = FEBRLReproducerLR(num_features=X_train_tensor.shape[1], inverse_reg=inverse_reg)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_lr.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Test the model\n",
    "    frl_output = febrl_reproducer_lr.predict(X_test_tensor).detach()\n",
    "\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frl_output])\n",
    "\n",
    "    print(\"inverse_reg = {}: {}\".format(inverse_reg, evaluation(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.3 Logistic Regression Training, Evaluation, and Bagging\n",
    "\n",
    "The reference implementation uses a 10-split k-fold as their bootstrapping technique. We will do the same here to ensure that our implementation is trained with the same data as the reference implementation. After each base learner is trained, it is evaluated with the test data set. After all base learners have evaluated the test data set, their outputs are passed through the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 0: {'no_false': 243, 'confusion_matrix': [2391, 35, 208, 360276], 'precision': 0.9855729596042869, 'sensitivity': 0.9199692189303579, 'no_links': 2426, 'F-score': 0.9516417910447761}\n",
      "Execution 1: {'no_false': 237, 'confusion_matrix': [2390, 28, 209, 360283], 'precision': 0.9884201819685691, 'sensitivity': 0.9195844555598307, 'no_links': 2418, 'F-score': 0.9527606139126967}\n",
      "Execution 2: {'no_false': 247, 'confusion_matrix': [2392, 40, 207, 360271], 'precision': 0.9835526315789473, 'sensitivity': 0.9203539823008849, 'no_links': 2432, 'F-score': 0.9509043927648578}\n",
      "Execution 3: {'no_false': 253, 'confusion_matrix': [2393, 47, 206, 360264], 'precision': 0.9807377049180328, 'sensitivity': 0.9207387456714121, 'no_links': 2440, 'F-score': 0.9497916253224846}\n",
      "Execution 4: {'no_false': 246, 'confusion_matrix': [2391, 38, 208, 360273], 'precision': 0.9843557019349527, 'sensitivity': 0.9199692189303579, 'no_links': 2429, 'F-score': 0.951073985680191}\n",
      "Execution 5: {'no_false': 250, 'confusion_matrix': [2393, 44, 206, 360267], 'precision': 0.9819450143619204, 'sensitivity': 0.9207387456714121, 'no_links': 2437, 'F-score': 0.9503574265289914}\n",
      "Execution 6: {'no_false': 255, 'confusion_matrix': [2393, 49, 206, 360262], 'precision': 0.9799344799344799, 'sensitivity': 0.9207387456714121, 'no_links': 2442, 'F-score': 0.9494147986510613}\n",
      "Execution 7: {'no_false': 255, 'confusion_matrix': [2393, 49, 206, 360262], 'precision': 0.9799344799344799, 'sensitivity': 0.9207387456714121, 'no_links': 2442, 'F-score': 0.9494147986510613}\n",
      "Execution 8: {'no_false': 251, 'confusion_matrix': [2393, 45, 206, 360266], 'precision': 0.9815422477440525, 'sensitivity': 0.9207387456714121, 'no_links': 2438, 'F-score': 0.9501687512408179}\n",
      "Execution 9: {'no_false': 245, 'confusion_matrix': [2392, 38, 207, 360273], 'precision': 0.9843621399176955, 'sensitivity': 0.9203539823008849, 'no_links': 2430, 'F-score': 0.9512825611453569}\n",
      "Logistic Regression bagging: {'no_false': 247, 'confusion_matrix': [2392, 40, 207, 360271], 'precision': 0.9835526315789473, 'sensitivity': 0.9203539823008849, 'no_links': 2432, 'F-score': 0.9509043927648578}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Logistic Regression Model / Training, Evaluation, and Bagging\n",
    "\n",
    "# Perform bagging across 10 models\n",
    "frl_kfold_count = 10\n",
    "frl_kfold = KFold(n_splits=frl_kfold_count, shuffle=True, random_state=12345)\n",
    "frl_kfold_i = 0\n",
    "\n",
    "frl_results = [0] * frl_kfold_count\n",
    "\n",
    "for train_indicies, _ in frl_kfold.split(X_train):\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_nn = FEBRLReproducerLR(num_features=X_train_tensor.shape[1], inverse_reg=frl_inverse_reg_optimal)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_nn.fit(X_train_tensor[train_indicies], y_train_tensor[train_indicies])\n",
    "\n",
    "    # Test the model\n",
    "    frl_results[frl_kfold_i] = febrl_reproducer_nn.predict(X_test_tensor).detach().numpy()\n",
    "\n",
    "    # Print the results of the current base learner for convenience\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frl_results[frl_kfold_i]])\n",
    "    print(\"Execution {}: {}\".format(frl_kfold_i, evaluation(y_test, y_pred)))\n",
    "\n",
    "    frl_kfold_i = frl_kfold_i + 1\n",
    "\n",
    "frl_bagging_raw_score = np.average(frl_results, axis=0)\n",
    "frl_bagging_binary_score = np.copy(frl_bagging_raw_score)\n",
    "frl_bagging_binary_score[frl_bagging_binary_score > 0.5] = 1\n",
    "frl_bagging_binary_score[frl_bagging_binary_score <= 0.5] = 0\n",
    "frl_bagging_evaluation = evaluation(y_test, frl_bagging_binary_score)\n",
    "print(\"Logistic Regression bagging: {}\".format(frl_bagging_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stacking\n",
    "\n",
    "The bagged base learners are stacked to produce a the model's final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble bagging-stacking: {'no_false': 251, 'confusion_matrix': [2388, 40, 211, 360271], 'precision': 0.9835255354200988, 'sensitivity': 0.9188149288187765, 'no_links': 2428, 'F-score': 0.9500696240302366}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Stacking / Ensemble Prediction\n",
    "\n",
    "fr_stacking_threshold = 0.99\n",
    "\n",
    "fr_stacking_binary_score = np.average([frs_bagging_binary_score, frn_bagging_binary_score, frl_bagging_binary_score], axis=0)\n",
    "fr_stacking_binary_score[fr_stacking_binary_score > fr_stacking_threshold] = 1\n",
    "fr_stacking_binary_score[fr_stacking_binary_score <= fr_stacking_threshold] = 0\n",
    "fr_stacking_evaluation = evaluation(y_test, fr_stacking_binary_score)\n",
    "print(\"Ensemble bagging-stacking: {}\".format(fr_stacking_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Reference Implementation Validation\n",
    "\n",
    "#### Base Learners\n",
    "\n",
    "Each of the following cells executes one of the three base learner models from the sklearn library used by the reference implementation with identical parameterization and data set as a sanity check to validate the paper's models' results using the same data set used by the reference implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Neural Network)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn MLPClassifier from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "nn_validation_model = MLPClassifier(solver='lbfgs', alpha=2000, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'relu',random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn_validation_model.fit(X_train, y_train)\n",
    "\n",
    "nn_validation_model_results = classify(nn_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper nn: {}\".format(evaluation(y_test, nn_validation_model_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Logistic Regression)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn LogisticRegression from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "lg_validation_model = LogisticRegression(C=0.005, penalty = 'l2',class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "lg_validation_model.fit(X_train, y_train)\n",
    "\n",
    "lg_validation_model_results = classify(lg_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper logistic regression: {}\".format(evaluation(y_test, lg_validation_model_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Support Vector Machine)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn SVC from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "svm_validation_model = svm.SVC(C = 0.001, kernel = 'linear')\n",
    "svm_validation_model.fit(X_train, y_train)\n",
    "\n",
    "svm_validation_model_results = classify(svm_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper support vector machine: {}\".format(evaluation(y_test, svm_validation_model_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
