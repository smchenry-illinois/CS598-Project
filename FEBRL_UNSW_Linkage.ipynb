{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce results of Scheme A\n",
    "\n",
    "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
    "\n",
    "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
    "\n",
    "February 2019\n",
    "\n",
    "Jounal of Biomedical Informatics\n",
    "\n",
    "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
    "\n",
    "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
    "\n",
    "February 2019\n",
    "\n",
    "Jounal of Biomedical Informatics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return int((f11 == f22) and (f12 == f21))\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name')\n",
    "    c.string('given_name_soundex', 'given_name_soundex', method='jarowinkler', label='y_name_soundex')\n",
    "    c.string('given_name_nysiis', 'given_name_nysiis', method='jarowinkler', label='y_name_nysiis')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname')\n",
    "    c.string('surname_soundex', 'surname_soundex', method='jarowinkler', label='y_surname_soundex')\n",
    "    c.string('surname_nysiis', 'surname_nysiis', method='jarowinkler', label='y_surname_nysiis')\n",
    "    c.exact('street_number', 'street_number', label='y_street_number')\n",
    "    c.string('address_1', 'address_1', method='levenshtein', threshold=0.7, label='y_address1')\n",
    "    c.string('address_2', 'address_2', method='levenshtein', threshold=0.7, label='y_address2')\n",
    "    c.exact('postcode', 'postcode', label='y_postcode')\n",
    "    c.exact('day', 'day', label='y_day')\n",
    "    c.exact('month', 'month', label='y_month')\n",
    "    c.exact('year', 'year', label='y_year')\n",
    "        \n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'svm': # Support Vector Machine\n",
    "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nb': # Naive Bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = 'febrl3_UNSW'\n",
    "testset = 'febrl4_UNSW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 5000 , number of matched pairs:  1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\recordlinkage\\preprocessing\\encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building X_train, y_train\n"
     ]
    }
   ],
   "source": [
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "df_train['given_name_soundex'] = phonetic(df_train['given_name'], method='soundex')\n",
    "df_train['given_name_nysiis'] = phonetic(df_train['given_name'], method='nysiis')\n",
    "df_train['surname_soundex'] = phonetic(df_train['surname'], method='soundex')\n",
    "df_train['surname_nysiis'] = phonetic(df_train['surname'], method='nysiis')\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import test set...\n",
      "Test set size: 10000 , number of matched pairs:  5000\n",
      "BLOCKING PERFORMANCE:\n",
      "Number of pairs of matched given_name: 154898 , detected  3287 /5000 true matched pairs, missed 1713\n",
      "Number of pairs of matched surname: 170843 , detected  3325 /5000 true matched pairs, missed 1675\n",
      "Number of pairs of matched postcode: 53197 , detected  4219 /5000 true matched pairs, missed 781\n",
      "Number of pairs of at least 1 field matched: 372073 , detected  4894 /5000 true matched pairs, missed 106\n"
     ]
    }
   ],
   "source": [
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "print(\"Import test set...\")\n",
    "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
    "\n",
    "print(\"BLOCKING PERFORMANCE:\")\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
    "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
    "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n",
      "Count labels of y_test: Counter({0: 367179, 1: 4894})\n",
      "Finished building X_test, y_test\n"
     ]
    }
   ],
   "source": [
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "df_test['given_name_soundex'] = phonetic(df_test['given_name'], method='soundex')\n",
    "df_test['given_name_nysiis'] = phonetic(df_test['given_name'], method='nysiis')\n",
    "df_test['surname_soundex'] = phonetic(df_test['surname'], method='soundex')\n",
    "df_test['surname_nysiis'] = phonetic(df_test['surname'], method='nysiis')\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
      "Model: nn , Param_1: relu , tuning range: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Temp\\ipykernel_16996\\1762433943.py:115: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = count_true_pos/(count_true_pos+count_false_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No_false: [9437, 12318, 10063, 10025, 9861, 14780, 7256, 5283, 3272, 1651, 718, 564, 495, 411, 398, 391, 421, 419, 4894, 367179] \n",
      "\n",
      "Precision: [0.3414089227117224, 0.28423622413392235, 0.32708904796949223, 0.32794581181677956, 0.33159286634569746, 0.24865249669480322, 0.40273431065722287, 0.4808694796891905, 0.5994116204952195, 0.7480495640201927, 0.8732570611369325, 0.8982707873436351, 0.9099720410065237, 0.9252513754505787, 0.9281904761904762, 0.9310740953475014, 0.9298481645204689, 0.9318664350511484, nan, 0.013153332813722038] \n",
      "\n",
      "Sensitivity: [0.9991826726604005, 0.9991826726604005, 0.9989783408255006, 0.9991826726604005, 0.9991826726604005, 0.9991826726604005, 0.9991826726604005, 0.9989783408255006, 0.9991826726604005, 0.9991826726604005, 0.9981610134859011, 0.9977523498161014, 0.9975480179812015, 0.996526358806702, 0.9957090314671025, 0.9936657131181038, 0.9885574172456069, 0.9865140988966081, 0.0, 1.0] \n",
      "\n",
      "F-score: [0.508924389863142, 0.4425739885962531, 0.49281790232347167, 0.49381469325927796, 0.49793798686421264, 0.3982084690553746, 0.5740784221648274, 0.6492264789854592, 0.7493104505056697, 0.8555681917592511, 0.931540808543097, 0.9454017424975799, 0.9517496832049906, 0.9595671421544515, 0.9607649842271294, 0.9613521794998516, 0.9583044468654056, 0.9584119106699751, nan, 0.0259651375319325] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "# Choose model\n",
    "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
    "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
    "modeltype_2 = 'relu'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
    "modelparam_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000] # C for svm, C for lg, alpha for NN\n",
    "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
    "precision = []\n",
    "sensitivity = []\n",
    "Fscore = []\n",
    "nb_false = []\n",
    "\n",
    "for modelparam in modelparam_range:\n",
    "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
    "    final_result = classify(md, X_test)\n",
    "    final_eval = evaluation(y_test, final_result)\n",
    "    precision += [final_eval['precision']]\n",
    "    sensitivity += [final_eval['sensitivity']]\n",
    "    Fscore += [final_eval['F-score']]\n",
    "    nb_false  += [final_eval['no_false']]\n",
    "    \n",
    "print(\"No_false:\",nb_false,\"\\n\")\n",
    "print(\"Precision:\",precision,\"\\n\")\n",
    "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
    "print(\"F-score:\", Fscore,\"\\n\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAGGING PERFORMANCE:\n",
      "\n",
      "svm per fold:\n",
      "Fold 0 {'no_false': 324, 'confusion_matrix': [4881, 311, 13, 366868], 'precision': 0.9401001540832049, 'sensitivity': 0.9973436861463016, 'no_links': 5192, 'F-score': 0.9678762641284949}\n",
      "Fold 1 {'no_false': 252, 'confusion_matrix': [4881, 239, 13, 366940], 'precision': 0.9533203125, 'sensitivity': 0.9973436861463016, 'no_links': 5120, 'F-score': 0.9748352306770521}\n",
      "Fold 2 {'no_false': 290, 'confusion_matrix': [4881, 277, 13, 366902], 'precision': 0.946297014346646, 'sensitivity': 0.9973436861463016, 'no_links': 5158, 'F-score': 0.971150019896538}\n",
      "Fold 3 {'no_false': 269, 'confusion_matrix': [4881, 256, 13, 366923], 'precision': 0.9501654662254234, 'sensitivity': 0.9973436861463016, 'no_links': 5137, 'F-score': 0.9731831322899013}\n",
      "Fold 4 {'no_false': 285, 'confusion_matrix': [4881, 272, 13, 366907], 'precision': 0.9472152144381913, 'sensitivity': 0.9973436861463016, 'no_links': 5153, 'F-score': 0.9716333233801134}\n",
      "Fold 5 {'no_false': 267, 'confusion_matrix': [4881, 254, 13, 366925], 'precision': 0.9505355404089582, 'sensitivity': 0.9973436861463016, 'no_links': 5135, 'F-score': 0.9733772061023033}\n",
      "Fold 6 {'no_false': 311, 'confusion_matrix': [4881, 298, 13, 366881], 'precision': 0.9424599343502607, 'sensitivity': 0.9973436861463016, 'no_links': 5179, 'F-score': 0.9691253846917502}\n",
      "Fold 7 {'no_false': 313, 'confusion_matrix': [4881, 300, 13, 366879], 'precision': 0.9420961204400695, 'sensitivity': 0.9973436861463016, 'no_links': 5181, 'F-score': 0.9689330024813896}\n",
      "Fold 8 {'no_false': 337, 'confusion_matrix': [4881, 324, 13, 366855], 'precision': 0.9377521613832853, 'sensitivity': 0.9973436861463016, 'no_links': 5205, 'F-score': 0.966630359441529}\n",
      "Fold 9 {'no_false': 276, 'confusion_matrix': [4881, 263, 13, 366916], 'precision': 0.9488724727838258, 'sensitivity': 0.9973436861463016, 'no_links': 5144, 'F-score': 0.972504482964734}\n",
      "svm bagging: {'no_false': 284, 'confusion_matrix': [4881, 271, 13, 366908], 'precision': 0.9473990683229814, 'sensitivity': 0.9973436861463016, 'no_links': 5152, 'F-score': 0.9717300418076846}\n",
      "\n",
      "nn per fold:\n",
      "Fold 0 {'no_false': 391, 'confusion_matrix': [4871, 368, 23, 366811], 'precision': 0.9297575873258256, 'sensitivity': 0.9953003677973028, 'no_links': 5239, 'F-score': 0.9614132043817231}\n",
      "Fold 1 {'no_false': 369, 'confusion_matrix': [4871, 346, 23, 366833], 'precision': 0.9336783592102741, 'sensitivity': 0.9953003677973028, 'no_links': 5217, 'F-score': 0.9635050934625655}\n",
      "Fold 2 {'no_false': 389, 'confusion_matrix': [4873, 368, 21, 366811], 'precision': 0.9297843922915474, 'sensitivity': 0.9957090314671025, 'no_links': 5241, 'F-score': 0.9616181549087321}\n",
      "Fold 3 {'no_false': 379, 'confusion_matrix': [4871, 356, 23, 366823], 'precision': 0.931892098718194, 'sensitivity': 0.9953003677973028, 'no_links': 5227, 'F-score': 0.9625531074004545}\n",
      "Fold 4 {'no_false': 387, 'confusion_matrix': [4871, 364, 23, 366815], 'precision': 0.9304680038204394, 'sensitivity': 0.9953003677973028, 'no_links': 5235, 'F-score': 0.9617928719518216}\n",
      "Fold 5 {'no_false': 397, 'confusion_matrix': [4872, 375, 22, 366804], 'precision': 0.9285305889079474, 'sensitivity': 0.9955046996322027, 'no_links': 5247, 'F-score': 0.9608519869835322}\n",
      "Fold 6 {'no_false': 424, 'confusion_matrix': [4871, 401, 23, 366778], 'precision': 0.923937784522003, 'sensitivity': 0.9953003677973028, 'no_links': 5272, 'F-score': 0.9582923470391501}\n",
      "Fold 7 {'no_false': 415, 'confusion_matrix': [4871, 392, 23, 366787], 'precision': 0.9255177655329659, 'sensitivity': 0.9953003677973028, 'no_links': 5263, 'F-score': 0.9591414787831052}\n",
      "Fold 8 {'no_false': 397, 'confusion_matrix': [4871, 374, 23, 366805], 'precision': 0.9286939942802669, 'sensitivity': 0.9953003677973028, 'no_links': 5245, 'F-score': 0.9608442647203866}\n",
      "Fold 9 {'no_false': 407, 'confusion_matrix': [4871, 384, 23, 366795], 'precision': 0.9269267364414843, 'sensitivity': 0.9953003677973028, 'no_links': 5255, 'F-score': 0.959897526849936}\n",
      "nn bagging: {'no_false': 396, 'confusion_matrix': [4871, 373, 23, 366806], 'precision': 0.9288710907704043, 'sensitivity': 0.9953003677973028, 'no_links': 5244, 'F-score': 0.9609390412310119}\n",
      "\n",
      "lg per fold:\n",
      "Fold 0 {'no_false': 472, 'confusion_matrix': [4882, 460, 12, 366719], 'precision': 0.9138899288655934, 'sensitivity': 0.9975480179812015, 'no_links': 5342, 'F-score': 0.9538882375928097}\n",
      "Fold 1 {'no_false': 428, 'confusion_matrix': [4882, 416, 12, 366763], 'precision': 0.9214798036995092, 'sensitivity': 0.9975480179812015, 'no_links': 5298, 'F-score': 0.9580062794348508}\n",
      "Fold 2 {'no_false': 487, 'confusion_matrix': [4882, 475, 12, 366704], 'precision': 0.9113309688258353, 'sensitivity': 0.9975480179812015, 'no_links': 5357, 'F-score': 0.9524924397619744}\n",
      "Fold 3 {'no_false': 454, 'confusion_matrix': [4882, 442, 12, 366737], 'precision': 0.9169797145003756, 'sensitivity': 0.9975480179812015, 'no_links': 5324, 'F-score': 0.9555686044235662}\n",
      "Fold 4 {'no_false': 456, 'confusion_matrix': [4882, 444, 12, 366735], 'precision': 0.9166353736387532, 'sensitivity': 0.9975480179812015, 'no_links': 5326, 'F-score': 0.9553816046966732}\n",
      "Fold 5 {'no_false': 490, 'confusion_matrix': [4882, 478, 12, 366701], 'precision': 0.9108208955223881, 'sensitivity': 0.9975480179812015, 'no_links': 5360, 'F-score': 0.9522137702360055}\n",
      "Fold 6 {'no_false': 500, 'confusion_matrix': [4882, 488, 12, 366691], 'precision': 0.9091247672253259, 'sensitivity': 0.9975480179812015, 'no_links': 5370, 'F-score': 0.9512860483242401}\n",
      "Fold 7 {'no_false': 486, 'confusion_matrix': [4882, 474, 12, 366705], 'precision': 0.9115011202389843, 'sensitivity': 0.9975480179812015, 'no_links': 5356, 'F-score': 0.9525853658536585}\n",
      "Fold 8 {'no_false': 474, 'confusion_matrix': [4882, 462, 12, 366717], 'precision': 0.9135479041916168, 'sensitivity': 0.9975480179812015, 'no_links': 5344, 'F-score': 0.9537018949013479}\n",
      "Fold 9 {'no_false': 489, 'confusion_matrix': [4882, 477, 12, 366702], 'precision': 0.910990856503079, 'sensitivity': 0.9975480179812015, 'no_links': 5359, 'F-score': 0.9523066419584512}\n",
      "lg bagging: {'no_false': 480, 'confusion_matrix': [4882, 468, 12, 366711], 'precision': 0.9125233644859813, 'sensitivity': 0.9975480179812015, 'no_links': 5350, 'F-score': 0.9531433033971106}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
    "\n",
    "print(\"BAGGING PERFORMANCE:\\n\")\n",
    "modeltypes = ['svm', 'nn', 'lg'] \n",
    "modeltypes_2 = ['linear', 'relu', 'l2']\n",
    "modelparams = [0.005, 100, 0.2]\n",
    "nFold = 10\n",
    "kf = KFold(n_splits=nFold)\n",
    "model_raw_score = [0]*3\n",
    "model_binary_score = [0]*3\n",
    "model_i = 0\n",
    "for model_i in range(3):\n",
    "    modeltype = modeltypes[model_i]\n",
    "    modeltype_2 = modeltypes_2[model_i]\n",
    "    modelparam = modelparams[model_i]\n",
    "    print(modeltype, \"per fold:\")\n",
    "    iFold = 0\n",
    "    result_fold = [0]*nFold\n",
    "    final_eval_fold = [0]*nFold\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
    "        result_fold[iFold] = classify(md, X_test)\n",
    "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
    "        print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
    "        iFold = iFold + 1\n",
    "    bagging_raw_score = np.average(result_fold, axis=0)\n",
    "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
    "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
    "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
    "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
    "    print(modeltype, \"bagging:\", bagging_eval)\n",
    "    print('')\n",
    "    model_raw_score[model_i] = bagging_raw_score\n",
    "    model_binary_score[model_i] = bagging_binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING PERFORMANCE:\n",
      "\n",
      "{'no_false': 213, 'confusion_matrix': [4870, 189, 24, 366990], 'precision': 0.9626408381102984, 'sensitivity': 0.9950960359624029, 'no_links': 5059, 'F-score': 0.9785994172611272}\n"
     ]
    }
   ],
   "source": [
    "thres = .99\n",
    "\n",
    "print(\"STACKING PERFORMANCE:\\n\")\n",
    "stack_raw_score = np.average(model_raw_score, axis=0)\n",
    "stack_binary_score = np.copy(stack_raw_score)\n",
    "stack_binary_score[stack_binary_score > thres] = 1\n",
    "stack_binary_score[stack_binary_score <= thres] = 0\n",
    "stacking_eval = evaluation(y_test, stack_binary_score)\n",
    "print(stacking_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS598 Project Code\n",
    "\n",
    "The following contain our own code for replicating and validating the results of the original paper. The data sets and therefore data preprocessing constructs from the original paper are reused here rather than being reimplemented.\n",
    "\n",
    "The code is organized into sections:\n",
    "1. Base Learners\n",
    "     1. Support Vector Machine\n",
    "     1. Neural Network\n",
    "     1. Linear Regression\n",
    "1. Bagging\n",
    "1. Stacking\n",
    "\n",
    "Following our code is an \"appendix\" section which performs out-of-context validation on the reference implementation's base learners' performance using the same parameterization and data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Support Vector Machine Base Learner (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Neural Network Base Learner\n",
    "\n",
    "##### 1.2.1. Neural Network Model Implementation\n",
    "\n",
    "The neural network implementation, FEBRLReproducerNN, reproduces the results of the neural network base learner from the original paper. This model has two initialization parameters:\n",
    "1. `num_features`: The number of features in the dataset; for the base dataset, this value is 13, but this value can differ if a dataset with fewer or additional features to be used.\n",
    "1. `weight_decay`: The hyperparameter that the paper explored via grid search to determine optimal weight decay for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS598 Project Code / Neural Network Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Reference implementation NN parameters\n",
    "class FEBRLReproducerNN(nn.Module):\n",
    "    def __init__(self, num_features, weight_decay=0.0):\n",
    "        # Create the our PyTorch nn model\n",
    "        super(FEBRLReproducerNN, self).__init__()\n",
    "\n",
    "        # STEP 1\n",
    "        # Specify parameters for our PyTorch nn model based upon the analogous\n",
    "        # parameters used by the original paper's sklearn MLPClassifier\n",
    "\n",
    "        # PyTorch nn concept                            Analogous sklearn MLPClassifier parameter\n",
    "        # ------------------                            -----------------------------------------\n",
    "        self.optimizer = None                           # solver (optimizer; original paper uses LBFGS, but we will use SGD (defined later) due to PyTorch-sklearn differences)\n",
    "        self.optimizer_weight_decay = weight_decay      # alpha (L2 penalty/regularization term)\n",
    "        self.num_hidden_layer_nodes = 256               # hidden_layer_sizes (tuple of hidden layer nodes)\n",
    "        self.activation = F.relu                        # activation (activation function)\n",
    "        self.random_state = 12345                       # random_state (static, random state for reproducibility)\n",
    "        #                                               # batch_size (minibatch size; unused in our model)\n",
    "        #                                               # learning_rate (tells the model to use the provided initial learning rate; n/a to our model)\n",
    "        self.optimizer_learning_rate_init = 0.001       # learning_rate_init (initial learning rate)\n",
    "        self.optimizer_dampening = 0.5                  # power_t (dampening)\n",
    "        self.num_max_epochs = 10000                     # max_iter (maximum number of epochs when using stochastic optimizers)\n",
    "        self.shuffle = True                             # shuffle (shuffle samples in each iteration)\n",
    "        self.tolerance = 0.0001                         # tol (optimization tolorance; early training termination)\n",
    "        #                                               # verbose (print model progress debug messages to console; specified by unused by original paper)\n",
    "        #                                               # warm_start (initialize the model with the results of previous executions; specified but unused by original paper)\n",
    "        self.optimizer_momentum = 0.9                   # momentum (optimizer momentum)\n",
    "        self.use_nesterov_momentum = True               # nesterovs_momentum (use Nesterov's momentum in the optimizer)\n",
    "        #                                               # early_stopping (terminate early when validation is not improving; 'False' in original paper)\n",
    "        #                                               # validation_fraction (validation data set criteria for early stopping; specified by unused by original paper)\n",
    "        #                                               # beta_1 (parameter for Adam optimizer; specified but unused by original paper)\n",
    "        #                                               # beta_2 (parameter for Adam optimizer; specified but unused by original paper)\n",
    "        #                                               # epsilon (parameter for Adam optimizer; specified but unused by original paper)\n",
    "\n",
    "        # STEP 2\n",
    "        # Define the layers for our nn model\n",
    "        self.num_input_features = num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.num_input_features, out_features=self.num_hidden_layer_nodes, bias=False)\n",
    "        self.fc2 = nn.Linear(in_features=self.num_hidden_layer_nodes, out_features=1, bias=False)\n",
    "\n",
    "        # STEP 3\n",
    "        # Define the criteria and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "            lr=self.optimizer_learning_rate_init,\n",
    "            weight_decay=self.optimizer_weight_decay,\n",
    "            momentum=self.optimizer_momentum,\n",
    "            dampening=0,\n",
    "            nesterov=self.use_nesterov_momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform a forward pass on the nn; it is not recommended to call this\n",
    "        # function directly, but to instead call fit(...) or predict(...) so that model's\n",
    "        # mode is correctly set automatically\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Train the nn with the specified parameters; analogous to sklearn's\n",
    "        # MLPClassifier.fit(...) method\n",
    "        self.train()\n",
    "\n",
    "        kfold = KFold(n_splits=10, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        loss_previous_epoch = 1.0\n",
    "        loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "        for epoch_i in np.arange(self.num_max_epochs):\n",
    "            loss = None\n",
    "\n",
    "            for train_indicies, test_indicies in kfold.split(X_train):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(X_train[train_indicies])\n",
    "                loss = self.criterion(output, y_train[train_indicies])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Determine if criteria for early training termination is satisfied\n",
    "            if (loss_previous_epoch - loss.item()) <= self.tolerance:\n",
    "                loss_consecutive_epochs_minimal = loss_consecutive_epochs_minimal + 1\n",
    "\n",
    "                if(loss_consecutive_epochs_minimal == 50):\n",
    "                    break\n",
    "            else:\n",
    "                loss_consecutive_epochs_minimal = 0\n",
    "\n",
    "            loss_previous_epoch = loss.item()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Test the nn with the specified parameters; analogous to sklearn's\n",
    "        # MLPClassifier.predict(...) method\n",
    "        self.eval()\n",
    "        return self.forward(X_test)\n",
    "\n",
    "# Convert the numpy training and testing sets to torch.tensor for us with the PyTorch library\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "frn_weight_decay_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000]  # 0.5 produces highest f1 score in our model\n",
    "frn_weight_decay_optimal = 0.5  # Determined through search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Neural Network Hyperparameter Search\n",
    "\n",
    "Grid search is performed on the set of candidate hyperparameters from the original paper using our model. Some hyperparameters may take a long time to test if they do not cause the model's loss to converge early (or at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay = 0.001: {'no_false': 526, 'confusion_matrix': [4865, 497, 29, 366682], 'precision': 0.9073107049608355, 'sensitivity': 0.9940743767879036, 'no_links': 5362, 'F-score': 0.9487129485179406}\n",
      "weight_decay = 0.002: {'no_false': 498, 'confusion_matrix': [4866, 470, 28, 366709], 'precision': 0.9119190404797601, 'sensitivity': 0.9942787086228034, 'no_links': 5336, 'F-score': 0.9513196480938416}\n",
      "weight_decay = 0.005: {'no_false': 568, 'confusion_matrix': [4857, 531, 37, 366648], 'precision': 0.9014476614699332, 'sensitivity': 0.9924397221087046, 'no_links': 5388, 'F-score': 0.9447578292161058}\n",
      "weight_decay = 0.01: {'no_false': 308, 'confusion_matrix': [4854, 268, 40, 366911], 'precision': 0.94767668879344, 'sensitivity': 0.991826726604005, 'no_links': 5122, 'F-score': 0.9692492012779552}\n",
      "weight_decay = 0.02: {'no_false': 358, 'confusion_matrix': [4855, 319, 39, 366860], 'precision': 0.938345574023966, 'sensitivity': 0.9920310584389048, 'no_links': 5174, 'F-score': 0.9644417957886373}\n",
      "weight_decay = 0.05: {'no_false': 337, 'confusion_matrix': [4858, 301, 36, 366878], 'precision': 0.9416553595658074, 'sensitivity': 0.9926440539436044, 'no_links': 5159, 'F-score': 0.9664776683577042}\n",
      "weight_decay = 0.1: {'no_false': 490, 'confusion_matrix': [4851, 447, 43, 366732], 'precision': 0.9156285390713477, 'sensitivity': 0.9912137310993052, 'no_links': 5298, 'F-score': 0.9519230769230769}\n",
      "weight_decay = 0.2: {'no_false': 404, 'confusion_matrix': [4837, 347, 57, 366832], 'precision': 0.9330632716049383, 'sensitivity': 0.988353085410707, 'no_links': 5184, 'F-score': 0.9599126810875174}\n",
      "weight_decay = 0.5: {'no_false': 262, 'confusion_matrix': [4805, 173, 89, 367006], 'precision': 0.9652470871836079, 'sensitivity': 0.9818144666939109, 'no_links': 4978, 'F-score': 0.9734602917341978}\n",
      "weight_decay = 1: {'no_false': 382, 'confusion_matrix': [4512, 0, 382, 367179], 'precision': 1.0, 'sensitivity': 0.9219452390682469, 'no_links': 4512, 'F-score': 0.9593876249202636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve McHenry\\AppData\\Local\\Temp\\ipykernel_16996\\1762433943.py:115: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = count_true_pos/(count_true_pos+count_false_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay = 5: {'no_false': 4894, 'confusion_matrix': [0, 0, 4894, 367179], 'precision': nan, 'sensitivity': 0.0, 'no_links': 0, 'F-score': nan}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\repos\\Medical-Record-Linkage-Ensemble\\FEBRL_UNSW_Linkage.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000019?line=5'>6</a>\u001b[0m febrl_reproducer_nn \u001b[39m=\u001b[39m FEBRLReproducerNN(num_features\u001b[39m=\u001b[39mX_train_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000019?line=7'>8</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000019?line=8'>9</a>\u001b[0m febrl_reproducer_nn\u001b[39m.\u001b[39;49mfit(X_train_tensor, y_train_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000019?line=10'>11</a>\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000019?line=11'>12</a>\u001b[0m frn_output \u001b[39m=\u001b[39m febrl_reproducer_nn\u001b[39m.\u001b[39mpredict(X_test_tensor)\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[1;32md:\\repos\\Medical-Record-Linkage-Ensemble\\FEBRL_UNSW_Linkage.ipynb Cell 17'\u001b[0m in \u001b[0;36mFEBRLReproducerNN.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000016?line=79'>80</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X_train[train_indicies])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000016?line=80'>81</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, y_train[train_indicies])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000016?line=81'>82</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000016?line=82'>83</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/Medical-Record-Linkage-Ensemble/FEBRL_UNSW_Linkage.ipynb#ch0000016?line=84'>85</a>\u001b[0m \u001b[39m# Determine if criteria for early training termination is satisfied\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Steve%20McHenry/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Neural Network Model\n",
    "\n",
    "# Perform nn base learner evaluation using the hyperparameter search range provided by the original paper\n",
    "for weight_decay in frn_weight_decay_range:\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_nn = FEBRLReproducerNN(num_features=X_train_tensor.shape[1], weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_nn.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Test the model\n",
    "    frn_output = febrl_reproducer_nn.predict(X_test_tensor).detach()\n",
    "\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frn_output])\n",
    "\n",
    "    print(\"weight_decay = {}: {}\".format(weight_decay, evaluation(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3. Neural Network Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 0: {'no_false': 224, 'confusion_matrix': [4806, 136, 88, 367043], 'precision': 0.9724807770133549, 'sensitivity': 0.9820187985288108, 'no_links': 4942, 'F-score': 0.9772265148434323}\n",
      "Execution 1: {'no_false': 229, 'confusion_matrix': [4807, 142, 87, 367037], 'precision': 0.9713073348151141, 'sensitivity': 0.9822231303637107, 'no_links': 4949, 'F-score': 0.9767347353449152}\n",
      "Execution 2: {'no_false': 240, 'confusion_matrix': [4806, 152, 88, 367027], 'precision': 0.9693424768051634, 'sensitivity': 0.9820187985288108, 'no_links': 4958, 'F-score': 0.9756394640682096}\n",
      "Execution 3: {'no_false': 279, 'confusion_matrix': [4805, 190, 89, 366989], 'precision': 0.9619619619619619, 'sensitivity': 0.9818144666939109, 'no_links': 4995, 'F-score': 0.9717868338557993}\n",
      "Execution 4: {'no_false': 277, 'confusion_matrix': [4807, 190, 87, 366989], 'precision': 0.9619771863117871, 'sensitivity': 0.9822231303637107, 'no_links': 4997, 'F-score': 0.9719947426953797}\n",
      "Execution 5: {'no_false': 287, 'confusion_matrix': [4804, 197, 90, 366982], 'precision': 0.9606078784243152, 'sensitivity': 0.981610134859011, 'no_links': 5001, 'F-score': 0.9709954522486105}\n",
      "Execution 6: {'no_false': 233, 'confusion_matrix': [4806, 145, 88, 367034], 'precision': 0.9707129872752979, 'sensitivity': 0.9820187985288108, 'no_links': 4951, 'F-score': 0.9763331640426612}\n",
      "Execution 7: {'no_false': 218, 'confusion_matrix': [4808, 132, 86, 367047], 'precision': 0.9732793522267207, 'sensitivity': 0.9824274621986105, 'no_links': 4940, 'F-score': 0.9778320113890583}\n",
      "Execution 8: {'no_false': 228, 'confusion_matrix': [4807, 141, 87, 367038], 'precision': 0.971503637833468, 'sensitivity': 0.9822231303637107, 'no_links': 4948, 'F-score': 0.9768339768339768}\n",
      "Execution 9: {'no_false': 232, 'confusion_matrix': [4805, 143, 89, 367036], 'precision': 0.9710994341147938, 'sensitivity': 0.9818144666939109, 'no_links': 4948, 'F-score': 0.9764275553749239}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Neural Network Model\n",
    "\n",
    "# Perform 10 train-test execution cycles\n",
    "for frn_i in np.arange(10):\n",
    "    # Create an instance of the feed-forward neural network\n",
    "    febrl_reproducer_nn = FEBRLReproducerNN(num_features=X_train_tensor.shape[1], weight_decay=frn_weight_decay_optimal)\n",
    "\n",
    "    # Train the model\n",
    "    febrl_reproducer_nn.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "    # Test the model\n",
    "    frn_output = febrl_reproducer_nn.predict(X_test_tensor).detach()\n",
    "\n",
    "    y_pred = np.asarray([1 if element > 0.5 else 0 for element in frn_output])\n",
    "\n",
    "    print(\"Execution {}: {}\".format(frn_i, evaluation(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Logistic Regression Base Learner (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bagging (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stacking (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Reference Implementation Validation\n",
    "\n",
    "#### Base Learners\n",
    "\n",
    "Each of the following cells executes one of the three base learner models from the sklearn library used by the reference implementation with identical parameterization and data set as a sanity check to validate the paper's models' results using the same data set used by the reference implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paper nn: {'no_false': 391, 'confusion_matrix': [4863, 360, 31, 366819], 'precision': 0.9310740953475014, 'sensitivity': 0.9936657131181038, 'no_links': 5223, 'F-score': 0.9613521794998516}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Neural Network)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn MLPClassifier from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "nn_validation_model = MLPClassifier(solver='lbfgs', alpha=200, hidden_layer_sizes=(256, ), \n",
    "                              activation = 'relu',random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn_validation_model.fit(X_train, y_train)\n",
    "\n",
    "nn_validation_model_results = classify(nn_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper nn: {}\".format(evaluation(y_test, nn_validation_model_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paper logistic regression: {'no_false': 488, 'confusion_matrix': [4882, 476, 12, 366703], 'precision': 0.9111608809257186, 'sensitivity': 0.9975480179812015, 'no_links': 5358, 'F-score': 0.9523995317986734}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Logistic Regression)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn LogisticRegression from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "lg_validation_model = LogisticRegression(C=0.2, penalty = 'l2',class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "lg_validation_model.fit(X_train, y_train)\n",
    "\n",
    "lg_validation_model_results = classify(lg_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper logistic regression: {}\".format(evaluation(y_test, lg_validation_model_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original paper support vector machine: {'no_false': 329, 'confusion_matrix': [4881, 316, 13, 366863], 'precision': 0.9391956898210506, 'sensitivity': 0.9973436861463016, 'no_links': 5197, 'F-score': 0.9673966901199089}\n"
     ]
    }
   ],
   "source": [
    "# CS598 Project Code / Reference Implementation Validation (Support Vector Machine)\n",
    "\n",
    "# Sanity check: Create, train, and test a new instance of an sklearn SVC from scratch using the original\n",
    "# paper's parameters to confirm that results are reproducible and absent any unexpected/hidden dependencies\n",
    "svm_validation_model = svm.SVC(C = 0.005, kernel = 'linear')\n",
    "svm_validation_model.fit(X_train, y_train)\n",
    "\n",
    "svm_validation_model_results = classify(svm_validation_model, X_test)\n",
    "\n",
    "print(\"Original paper support vector machine: {}\".format(evaluation(y_test, svm_validation_model_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
